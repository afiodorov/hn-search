#!/usr/bin/env python
"""
Script to fetch new HN comments from BigQuery, generate embeddings using MPS (Mac GPU),
and upsert them into partitioned PostgreSQL tables.

Idempotent and resumable - can be interrupted and restarted at any point.

Usage:
    uv run python misc/fetch_and_embed_new_comments.py [--resume] [--skip-fetch] [--skip-embed] [--skip-upsert]
"""

import os
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Optional, Tuple

import pandas as pd
import psycopg
from dotenv import load_dotenv
from google.cloud import bigquery
import torch
from sentence_transformers import SentenceTransformer
import html2text
import numpy as np
from pgvector.psycopg import register_vector

from hn_search.db_config import get_db_config

load_dotenv()

STATE_FILE = "data/raw/fetch_state.json"


def load_state() -> dict:
    """Load state from file"""
    state_path = Path(STATE_FILE)
    if state_path.exists():
        with open(state_path, 'r') as f:
            return json.load(f)
    return {}


def save_state(state: dict):
    """Save state to file"""
    state_path = Path(STATE_FILE)
    state_path.parent.mkdir(parents=True, exist_ok=True)
    with open(state_path, 'w') as f:
        json.dump(state, f, indent=2)
    print(f"üíæ State saved to {state_path}")


def strip_html(text: str) -> str:
    """Clean HTML from text"""
    h = html2text.HTML2Text()
    h.ignore_links = True
    h.ignore_images = True
    h.ignore_emphasis = True
    return h.handle(text).strip()


def get_connection():
    """Get a fresh database connection with vector registration"""
    db_config = get_db_config()
    conn = psycopg.connect(**db_config)
    register_vector(conn)
    return conn


def find_latest_nonempty_partition():
    """Find the latest non-empty partitioned table"""
    conn = get_connection()
    with conn.cursor() as cur:
        cur.execute("""
            SELECT tablename
            FROM pg_tables
            WHERE schemaname = 'public'
            AND tablename LIKE 'hn_documents_____%%'
            ORDER BY tablename DESC
        """)
        tables = [row[0] for row in cur.fetchall()]

        # Check each table for rows
        for table in tables:
            cur.execute(f"SELECT COUNT(*) FROM {table}")
            count = cur.fetchone()[0]
            if count > 0:
                conn.close()
                print(f"Found latest non-empty partition: {table} ({count:,} rows)")
                return table

    conn.close()
    return None


def get_max_id_from_partition(table_name):
    """Get the maximum id from the specified partition"""
    conn = get_connection()
    with conn.cursor() as cur:
        # IDs are stored as text, so we need to cast to bigint for proper comparison
        cur.execute(f"SELECT MAX(CAST(id AS BIGINT)) FROM {table_name}")
        max_id = cur.fetchone()[0]
    conn.close()
    print(f"Max ID in {table_name}: {max_id}")
    return max_id


def fetch_from_bigquery(min_id, output_dir="data/raw", state=None, project=None) -> Optional[Path]:
    """Fetch new comments from BigQuery starting after min_id"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Check if we already have a raw file from a previous run
    if state and state.get("raw_file"):
        raw_file = Path(state["raw_file"])
        if raw_file.exists():
            print(f"‚úÖ Found existing raw file: {raw_file}")
            return raw_file

    print(f"\nüì• Fetching comments from BigQuery with id > {min_id}")

    # Initialize client - uses Application Default Credentials by default
    # Specify project to control billing
    project = project or os.getenv("GOOGLE_CLOUD_PROJECT")
    client = bigquery.Client(project=project)

    print(f"üí∞ Using GCP project: {client.project}")
    print(f"   Account: Run 'gcloud auth list' to see active account")
    print(f"   Tip: Set billing alerts at https://console.cloud.google.com/billing/")

    query = f"""
    SELECT
      id,
      `by` AS author,
      `type`,
      text,
      timestamp
    FROM
      `bigquery-public-data.hacker_news.full`
    WHERE
      dead IS NOT TRUE
      AND deleted IS NOT TRUE
      AND text IS NOT NULL
      AND type = 'comment'
      AND id > {min_id}
    ORDER BY id
    """

    print("Running BigQuery query...")
    query_job = client.query(query)
    df = query_job.to_dataframe()

    print(f"Fetched {len(df):,} new comments")

    if len(df) == 0:
        print("No new comments to process")
        return None

    # Use consistent filename for resumability
    filename = f"new_comments_from_{min_id}.parquet"
    filepath = output_path / filename

    # Save atomically with temp file
    temp_filepath = filepath.with_suffix('.parquet.tmp')
    df.to_parquet(temp_filepath, index=False)
    temp_filepath.rename(filepath)

    print(f"üíæ Saved to {filepath}")

    return filepath


def generate_embeddings_mps(parquet_file, state=None) -> Tuple[Optional[Path], Optional[pd.DataFrame]]:
    """Generate embeddings using MPS (Mac GPU) - saves progress incrementally"""
    output_file = Path(str(parquet_file).replace(".parquet", "_embedded.parquet"))

    # Check if embeddings already exist
    if output_file.exists():
        print(f"‚úÖ Found existing embedded file: {output_file}")
        df = pd.read_parquet(output_file)
        return output_file, df

    # Check for MPS availability
    if torch.backends.mps.is_available():
        device = "mps"
        print("üöÄ Using MPS (Apple Silicon GPU)")
    elif torch.cuda.is_available():
        device = "cuda"
        print(f"üöÄ Using CUDA GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = "cpu"
        print("‚ö†Ô∏è  Using CPU (no GPU available)")

    model = SentenceTransformer(
        "sentence-transformers/all-mpnet-base-v2",
        device=device
    )

    print(f"\nüîÑ Loading {parquet_file}")
    df = pd.read_parquet(parquet_file)
    df = df[df["text"].notna() & (df["text"] != "")]

    print(f"Processing {len(df):,} documents")

    # Clean text
    df["clean_text"] = df["text"].astype(str).apply(strip_html)
    df = df[df["clean_text"].str.len() > 0]

    if len(df) == 0:
        print("No valid documents after cleaning")
        return None, None

    # Generate embeddings in chunks and save incrementally
    chunk_size = 1000  # Save every 1000 documents
    encode_batch_size = 128

    documents = df["clean_text"].tolist()
    all_embeddings = []

    temp_output = output_file.with_suffix('.parquet.tmp')

    for chunk_start in range(0, len(documents), chunk_size):
        chunk_end = min(chunk_start + chunk_size, len(documents))
        chunk_docs = documents[chunk_start:chunk_end]

        print(f"\nüì¶ Processing chunk {chunk_start//chunk_size + 1}/{(len(documents)-1)//chunk_size + 1}")

        chunk_embeddings = []
        for batch_start in range(0, len(chunk_docs), encode_batch_size):
            batch = chunk_docs[batch_start:batch_start+encode_batch_size]
            print(f"  Encoding batch {batch_start//encode_batch_size + 1}/{(len(chunk_docs)-1)//encode_batch_size + 1}")

            embeddings = model.encode(
                batch,
                batch_size=encode_batch_size,
                show_progress_bar=False,
                convert_to_numpy=True,
            )
            chunk_embeddings.extend(embeddings)

        all_embeddings.extend(chunk_embeddings)

        # Save intermediate progress
        partial_df = df.iloc[:chunk_end].copy()
        partial_df["embedding"] = [emb.tolist() for emb in all_embeddings]
        partial_df.to_parquet(temp_output, index=False)
        print(f"  üíæ Saved progress: {chunk_end}/{len(documents)} documents")

    # Final save - rename temp to final
    df["embedding"] = [emb.tolist() for emb in all_embeddings]
    temp_output.rename(output_file)
    print(f"\n‚úÖ Saved all embeddings to {output_file}")

    return output_file, df


def get_partition_name_for_timestamp(timestamp):
    """Get partition table name for a given timestamp"""
    # Convert timestamp to datetime if it's a string
    if isinstance(timestamp, str):
        dt = pd.to_datetime(timestamp)
    else:
        dt = timestamp

    return f"hn_documents_{dt.year:04d}_{dt.month:02d}"


def upsert_to_db(embedded_parquet_file, df, state=None):
    """Upsert embeddings into partitioned tables with batch processing"""
    print(f"\nüîÑ Upserting {len(df):,} documents to database")

    conn = get_connection()

    # Group by partition
    df["partition"] = df["timestamp"].apply(get_partition_name_for_timestamp)

    # Track processed IDs if resuming
    processed_ids = set(state.get("upserted_ids", [])) if state else set()

    total_inserted = 0
    batch_size = 500  # Insert in batches for better performance

    try:
        for partition_name, partition_df in df.groupby("partition"):
            print(f"\n  üìã Processing partition {partition_name} ({len(partition_df):,} rows)")

            # Get existing IDs from the database to avoid duplicates
            with conn.cursor() as cur:
                partition_ids = partition_df["id"].astype(str).tolist()
                cur.execute(
                    f"SELECT id FROM {partition_name} WHERE id = ANY(%s)",
                    (partition_ids,)
                )
                existing_ids = set(row[0] for row in cur.fetchall())
                print(f"  Found {len(existing_ids):,} existing IDs in {partition_name}")

            # Filter out already processed rows and existing IDs
            partition_df = partition_df[
                ~partition_df["id"].astype(str).isin(processed_ids | existing_ids)
            ]

            if len(partition_df) == 0:
                print(f"  ‚è≠Ô∏è  All rows already exist in {partition_name}")
                continue

            print(f"  Inserting {len(partition_df):,} new rows")

            records = [
                (
                    str(row["id"]),
                    row["clean_text"],
                    str(row["author"]),
                    str(row["timestamp"]),
                    str(row["type"]),
                    row["embedding"],
                )
                for _, row in partition_df.iterrows()
            ]

            # Process in batches
            for batch_start in range(0, len(records), batch_size):
                batch = records[batch_start:batch_start+batch_size]

                with conn.cursor() as cur:
                    cur.executemany(
                        f"""
                        INSERT INTO {partition_name} (id, clean_text, author, timestamp, type, embedding)
                        VALUES (%s, %s, %s, %s, %s, %s)
                        """,
                        batch,
                    )

                conn.commit()

                # Track progress
                batch_ids = [r[0] for r in batch]
                processed_ids.update(batch_ids)

                total_inserted += len(batch)
                print(f"    Progress: {batch_start + len(batch)}/{len(records)} | Total: {total_inserted:,}")

                # Save state periodically
                if state is not None:
                    state["upserted_ids"] = list(processed_ids)
                    save_state(state)

            print(f"  ‚úÖ Inserted {len(records):,} rows into {partition_name}")

    finally:
        conn.close()

    print(f"\n‚úÖ Total inserted: {total_inserted:,} documents")


def main():
    """Main execution flow with resumability"""
    parser = argparse.ArgumentParser(
        description="Fetch, embed, and upsert HN comments (idempotent & resumable)"
    )
    parser.add_argument("--resume", action="store_true", help="Resume from previous state")
    parser.add_argument("--skip-fetch", action="store_true", help="Skip BigQuery fetch step")
    parser.add_argument("--skip-embed", action="store_true", help="Skip embedding generation step")
    parser.add_argument("--skip-upsert", action="store_true", help="Skip database upsert step")
    parser.add_argument("--reset", action="store_true", help="Reset state and start fresh")
    parser.add_argument("--project", type=str, help="GCP project ID for BigQuery billing (defaults to GOOGLE_CLOUD_PROJECT env var or gcloud default)")

    args = parser.parse_args()

    print("=" * 80)
    print("HN Comments Fetcher, Embedder, and Upserter (Idempotent)")
    print("=" * 80)

    # Load or initialize state
    if args.reset:
        print("üîÑ Resetting state...")
        state = {}
    else:
        state = load_state()
        if state:
            print(f"üìÇ Loaded state from {STATE_FILE}")
            print(f"   Raw file: {state.get('raw_file', 'None')}")
            print(f"   Embedded file: {state.get('embedded_file', 'None')}")
            print(f"   Upserted IDs: {len(state.get('upserted_ids', []))} documents")

    # Step 1: Find latest non-empty partition
    latest_partition = find_latest_nonempty_partition()
    if not latest_partition:
        print("‚ùå No partitioned tables found!")
        return

    # Step 2: Get max ID from that partition
    max_id = get_max_id_from_partition(latest_partition)
    if max_id is None:
        print("‚ùå Could not determine max ID")
        return

    state["max_id"] = max_id
    save_state(state)

    # Step 3: Fetch new comments from BigQuery
    parquet_file = None
    if not args.skip_fetch:
        parquet_file = fetch_from_bigquery(max_id, state=state, project=args.project)
        if not parquet_file:
            print("No new data to process")
            return
        state["raw_file"] = str(parquet_file)
        save_state(state)
    else:
        print("‚è≠Ô∏è  Skipping fetch step")
        if state.get("raw_file"):
            parquet_file = Path(state["raw_file"])
        else:
            print("‚ùå No raw file in state - cannot skip fetch")
            return

    # Step 4: Generate embeddings
    embedded_file = None
    df = None
    if not args.skip_embed:
        embedded_file, df = generate_embeddings_mps(parquet_file, state=state)
        if not embedded_file:
            print("‚ùå Embedding generation failed")
            return
        state["embedded_file"] = str(embedded_file)
        save_state(state)
    else:
        print("‚è≠Ô∏è  Skipping embed step")
        if state.get("embedded_file"):
            embedded_file = Path(state["embedded_file"])
            df = pd.read_parquet(embedded_file)
        else:
            print("‚ùå No embedded file in state - cannot skip embed")
            return

    # Step 5: Upsert to database
    if not args.skip_upsert:
        upsert_to_db(embedded_file, df, state=state)
        state["completed"] = True
        state["completed_at"] = datetime.now().isoformat()
        save_state(state)
    else:
        print("‚è≠Ô∏è  Skipping upsert step")

    print("\n" + "=" * 80)
    print("‚úÖ All done!")
    print("=" * 80)


if __name__ == "__main__":
    main()
